# -*- coding: utf-8 -*-
"""STROKE RISK PREDICTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eD5eqp7M_juUW0xovZVtNx2mFrC33Aa1

STROKE RISK PREDICTION (STEP BY STEP ML PROJECT)
- 1st Step - EDA
(analyse the data for distribution, outliers and anomalies in the dataset. It includes analysing the data to find the distribution of data, its main characteristics, identifying patterns and visualizations. It also provides tools for hypothesis generation by visualizing and understanding the data through graphical representation)
- 2nd Step - Feature Engineering
(FE is simply using your existing knowledge of the dataset to create new features that can help a machine learning model perform better)- Handle the Missing Values if any, handling the Outliers, handling the Categorical Data, normalizing the Data for further Model building.
- 3rd Step - Feature Selection
- 4th Step - Building an ML model
- 5th Step - Checking the accuracy and performing hyperparameter tuning to enhance the performance

WAYS TO HANDLE MISSING VALUES:
(often encoded as NaNs, blanks or any other placeholders)
- Deletion
- Impute missing values with Mean/Median/Mode
- Prediction Model
- KNN Imputer
The command such as df.isnull().sum() prints the column with missing value.
fillna() method on the data frame is used for imputing missing values with mean, median, mode or constant value.
The goal is to find out which is a better measure of the central tendency of data and use that value for replacing missing values appropriately.

Plots such as box plots and distribution plots come very handy in deciding which techniques to use.
You can use the following code to print different plots such as box and distribution plots.

import seaborn as sns

Box plot

sns.boxplot(df.salary)

Distribution plot

sns.histplot(df.salary)

If data is skewed - outliers may affect mean and hence missing values shouldn't be imputed with mean
If data is symmetric - one can use mean for imputation
mean imputation - df.fillna(df.mean())
median imputation - df.fillna(df.median())
mode imputation - df['salary'] = df['salary'].fillna(df['salary'].mode()[0]) (can be done with categorical data)
OUTLIERS
- It increases the error variance and reduces the power of statistical tests
- If the outliers are non-randomly distributed, they can decrease normality
- They can bias or influence estimates that may be of substantive interest
- They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.

DOUBT :
- XgBoost stands for Extreme Gradient Boosting (for handling outliers)
- categorical features are handled using Encoder (turns them into numeric codes)
- from sklearn.preprocessing import LabelEncoder
- enc=LabelEncoder()
- Handling Imabalanced data (using SMOTE technique)

IMPORT LIBRARIES (pandas for dataframe, matplotlib and seaborn for data visualization)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt # plt is a common alias of matplotlib.pyplot
# Pyplot is a collection of functions in the popular visualization package Matplotlib.
# Its functions manipulate elements of a figure, such as creating a figure, creating a plotting area, plotting lines,
# adding plot labels, etc.
# %matplotlib inline
# It is a magic function that renders the figure in a notebook
# (instead of displaying a dump of the figure object)
import seaborn as sns
# Seaborn is an open-source Python library built on top of matplotlib. It is used for data visualization and exploratory
# data analysis. Seaborn works easily with dataframes and the Pandas library.
plt.rcParams['figure.figsize'] = (5, 5)

"""IMPORT DATAFRAME (remember to use / slash instead of \ )"""

df = pd.read_csv("/Users/eeshaiyer/Downloads/healthcare-dataset-stroke-data.csv")
#GET THE FIRST 5 ROWS:
df.head()

#GET THE LIST OF COLUMNS IN DATASET:
df.columns

#GET THE STATISTICS OF DATA:
df.describe()

"""The count of "BMI" is less compared to other features. Hence, BMI has some null values"""

# SEE THE DISTRIBUTION OF BMI
plt.subplots(figsize=(15,5))
sns.histplot(df['bmi'], color = 'cyan')
plt.title('Distribution of bmi', fontsize = 20)
plt.show()

df.isnull().sum() # returns the number of missing values in the data set

# WE ARE REPLACING THE NULL VALUES WITH MEAN OF THAT FEATURE (since the dist. is more or less symmetric as seen in the figure)
df['bmi'].fillna(df['bmi'].mean(),inplace=True)
# check again
df.isnull().sum()

# Plot histograms of each parameter

df.hist(figsize = (10, 10))
plt.show()

#visualize the correlation
plt.figure(figsize=(10,5))
sns.heatmap(df.corr(), annot=True, cmap = 'Wistia')
plt.show()

df.info()

"""Categorical data are - gender, ever_married, work_type, Residence_type ad smoking_status. These need to be handled (converted into numeric values)"""

from sklearn.preprocessing import LabelEncoder
enc=LabelEncoder()

gender=enc.fit_transform(df['gender'])
smoking_status=enc.fit_transform(df['smoking_status'])
work_type=enc.fit_transform(df['work_type'])
Residence_type=enc.fit_transform(df['Residence_type'])
ever_married=enc.fit_transform(df['ever_married'])

df['ever_married']=ever_married
df['Residence_type']=Residence_type
df['smoking_status']=smoking_status
df['gender']=gender
df['work_type']=work_type

df[['ever_married', 'Residence_type', 'smoking_status', 'gender', 'work_type']].head()

# check again
df.info()

df['stroke'].value_counts().plot.bar(color = 'red')
# we can call the value_counts method on the stroke field to see the count of unique values for each stroke type [ 0 and 1 ].
plt.title('Comparison of stroke feature')
plt.xlabel('zero & one')
plt.ylabel('count')
plt.show()

"""The data is completely imbalanced and needs to be handled."""

# SPLIT DATASET INTO X AND Y (FEATURES AND TARGET)
X = df.drop('stroke', axis=1)
y = df['stroke']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

# HANDLING IMBALANCED DATASET USING SMOTE TECHNIQUE (remember to pip install imblearn beforehand)
from imblearn.over_sampling import SMOTE

print("Before OverSampling, counts of label '1': {}".format(sum(y_train==1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train==0)))

sm = SMOTE(random_state=2)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())

print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train_res==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res==0)))

"""Model is now balanced"""

# MODEL BUILDING
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, classification_report

import numpy as np
rdf_model = RandomForestClassifier()
rdf_model.fit(X_train_res, y_train_res)
print('Training Score: {}'.format(rdf_model.score(X_train_res, y_train_res)))
print('Test Score: {}'.format(rdf_model.score(X_test, y_test)))

# MAKE PREDICTIONS
y_pred_test = rdf_model.predict(X_test)
# view accuracy score
accuracy_score(y_test, y_pred_test)

"""Accuracy is not a great measure of classifier performance when the classes are imbalanced."""

# VIEW CONFUSION MATRIX FOR TEST DATA AND PREDICTIONS
confusion_matrix(y_test, y_pred_test)

# VIEW CLASSIFICATION REPORT
print(classification_report(y_test, y_pred_test))

"""The metrics for label 0 are performing well. However, the model has low precision, recall and an F1 score for the label 1."""